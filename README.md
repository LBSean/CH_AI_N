# CH_AI_N

A production-grade AI agent platform built on LangGraph, LlamaIndex, and FastAPI.
Drop-in foundation for any AI-powered application â€” multi-tenant, memory-aware, fully observable.

---

## Are you ready to build?

**Yes.** The backend is fully operational:

| Capability | Status |
|---|---|
| Auth (register / login / JWT) | âœ… |
| LLM routing with fallback | âœ… |
| Streaming SSE responses | âœ… |
| Multi-turn conversation memory | âœ… |
| RAG with hybrid retrieval (vector + BM25) | âœ… |
| Episodic memory (cross-session recall) | âœ… |
| Tool calling (agentic loop) | âœ… |
| Async document ingestion (Celery) | âœ… |
| Per-user budget + rate limiting | âœ… |
| Cost tracking (per model, per user) | âœ… |
| Structured logging (structlog) | âœ… |
| Admin metrics API | âœ… |
| Postgres Row-Level Security | âœ… |
| Prompt injection filter | âœ… |
| Frontend chat UI | âœ… |
| Frontend auth UI | ðŸ”œ Phase 5 |
| Test suite | ðŸ”œ Phase 5 |

---

## Stack

```
Model Layer       LiteLLM          â€” model routing, fallback, cost tracking
Agent Runtime     LangGraph        â€” orchestration, state, tool calling
Tool Adapters     LangChain        â€” tool wrappers, LLM adapters
Knowledge         LlamaIndex       â€” ingestion, chunking, hybrid retrieval
Relational Store  PostgreSQL 16    â€” users, conversations, messages, episodic memory
Vector Store      pgvector         â€” semantic memory, document embeddings
Cache / Queue     Redis 7          â€” rate limiting, Celery broker
Background Worker Celery           â€” async ingestion, episodic summarisation
Observability     LangSmith        â€” LLM trace inspection
Logging           structlog        â€” structured JSON application logs
Backend           FastAPI          â€” API, auth, streaming
Frontend          Next.js 15       â€” chat UI, SSE proxy
Deployment        Railway + Vercel â€” backend + frontend
```

---

## Prerequisites

| Tool | Version | Purpose |
|---|---|---|
| Docker Desktop | Latest | Runs the full dev stack |
| Node.js | 20+ | Frontend dev server |
| Python | 3.11+ | Local tooling (Alembic, etc.) |
| `gh` CLI | Latest | GitHub operations |
| OpenAI API key | â€” | Required |
| Anthropic API key | â€” | Optional (fallback model) |
| LangSmith API key | â€” | Optional (tracing) |

---

## Quick Start

### 1. Clone and configure

```bash
git clone https://github.com/LBSean/CH_AI_N.git
cd CH_AI_N

cp .env.example .env
# Edit .env â€” fill in OPENAI_API_KEY, LITELLM_MASTER_KEY, JWT_SECRET at minimum
```

**Minimum required `.env` values:**
```bash
OPENAI_API_KEY=sk-...
LITELLM_MASTER_KEY=sk-any-string-you-choose
JWT_SECRET=$(openssl rand -hex 32)   # or any long random string
```

### 2. Start the stack

```bash
docker compose up --build
```

Services that come up:

| Service | URL | Purpose |
|---|---|---|
| FastAPI backend | http://localhost:8000 | Main API + docs at `/docs` |
| Next.js frontend | http://localhost:3000 | Chat UI |
| LiteLLM proxy | http://localhost:4000 | LLM gateway (dev) |
| Flower | http://localhost:5555 | Celery task monitor |
| Flowise | http://localhost:3001 | RAG prototyping (optional) |

### 3. Start the frontend (separate terminal)

```bash
cd frontend
cp .env.local.example .env.local
# Set NEXT_PUBLIC_API_URL=http://localhost:8000
npm install
npm run dev
```

### 4. Register and start building

```bash
# Register a user
curl -X POST http://localhost:8000/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{"email": "you@example.com", "password": "yourpassword"}'

# Response includes access_token â€” use it on all subsequent requests
```

---

## API Reference

All protected routes require `Authorization: Bearer <access_token>`.

### Auth

| Method | Path | Description |
|---|---|---|
| `POST` | `/api/auth/register` | Create account â†’ returns JWT pair |
| `POST` | `/api/auth/login` | Login â†’ returns JWT pair |
| `POST` | `/api/auth/refresh` | Refresh access token |

### Agent

| Method | Path | Description |
|---|---|---|
| `POST` | `/api/agent/invoke` | Synchronous agent call |
| `POST` | `/api/agent/stream` | Streaming SSE agent call |
| `GET` | `/api/agent/threads/{thread_id}` | Retrieve conversation state |

**Stream request:**
```bash
curl -X POST http://localhost:8000/api/agent/stream \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "What is LangGraph?", "thread_id": null}'
```

**Resume a conversation** by passing the `thread_id` from the first response:
```json
{ "message": "Tell me more", "thread_id": "abc-123" }
```

### Ingestion

| Method | Path | Description |
|---|---|---|
| `POST` | `/api/ingest/file` | Upload a `.txt` file (queued async) |
| `POST` | `/api/ingest/text` | Ingest raw text + metadata |

```bash
# Ingest text
curl -X POST http://localhost:8000/api/ingest/text \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"text": "LangGraph is a library for building stateful agents.", "metadata": {}}'

# Returns: {"status": "queued", "task_id": "..."}
# Watch progress at http://localhost:5555
```

### Admin

| Method | Path | Description |
|---|---|---|
| `GET` | `/api/admin/metrics` | Cost, usage, and budget for the current user |

### Health

| Method | Path | Description |
|---|---|---|
| `GET` | `/health` | DB connectivity check |

Full interactive API docs: **http://localhost:8000/docs**

---

## Environment Variables

| Variable | Required | Default | Description |
|---|---|---|---|
| `OPENAI_API_KEY` | âœ… | â€” | OpenAI provider key |
| `ANTHROPIC_API_KEY` | â€” | â€” | Anthropic fallback key |
| `LITELLM_MASTER_KEY` | âœ… | â€” | Internal API key for LiteLLM gateway |
| `LITELLM_MODE` | â€” | `proxy` | `proxy` (dev) or `library` (prod, no extra container) |
| `JWT_SECRET` | âœ… | â€” | Secret for signing JWTs â€” use `openssl rand -hex 32` |
| `DATABASE_URL` | âœ… | set in compose | PostgreSQL connection string |
| `REDIS_HOST` | â€” | `redis` | Redis hostname |
| `REDIS_PASSWORD` | â€” | â€” | Redis password (Upstash in prod) |
| `LANGCHAIN_API_KEY` | â€” | â€” | LangSmith key â€” tracing disabled if absent |
| `LANGCHAIN_PROJECT` | â€” | `ai-workspace` | LangSmith project name |

---

## Project Structure

```
CH_AI_N/
â”œâ”€â”€ backend/                    # FastAPI + LangGraph + LlamaIndex
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py        # memory_injection, agent, router
â”‚   â”‚   â”‚   â”œâ”€â”€ research_agent.py  # graph topology
â”‚   â”‚   â”‚   â””â”€â”€ tools.py        # LangChain tool definitions
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ admin.py        # /api/admin/metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py        # /api/agent/* endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py       # /health
â”‚   â”‚   â”‚   â””â”€â”€ ingest.py       # /api/ingest/*
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ deps.py         # get_current_user dependency
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py       # Pydantic schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ router.py       # /api/auth/* endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py     # bcrypt + JWT
â”‚   â”‚   â”‚   â””â”€â”€ service.py      # DB operations
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpointer.py # LangGraph Postgres checkpointer
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py       # Settings (pydantic-settings)
â”‚   â”‚   â”‚   â”œâ”€â”€ db.py           # Async psycopg3 helper
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_state.py  # AgentState TypedDict
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py          # LLM factory (proxy/library mode)
â”‚   â”‚   â”‚   â””â”€â”€ logging.py      # structlog configuration
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â”œâ”€â”€ episodic.py     # query + store episodic memory
â”‚   â”‚   â”‚   â””â”€â”€ tool_state.py   # JSONB tool state persistence
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ budget.py       # Token budget enforcement
â”‚   â”‚   â”‚   â”œâ”€â”€ rate_limit.py   # Redis sliding-window rate limiter
â”‚   â”‚   â”‚   â””â”€â”€ sanitize.py     # Prompt injection filter
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py     # Hybrid retrieval (vector + BM25)
â”‚   â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”‚   â”œâ”€â”€ celery_app.py   # Celery instance + queue config
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py        # ingest, summarize_episode, cleanup
â”‚   â”‚   â””â”€â”€ main.py             # FastAPI app entrypoint
â”‚   â”œâ”€â”€ alembic/                # Schema migrations
â”‚   â”œâ”€â”€ alembic.ini
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ frontend/                   # Next.js 15 + React 19
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ api/chat/       # SSE proxy â†’ FastAPI
â”‚       â”‚   â””â”€â”€ page.tsx
â”‚       â””â”€â”€ components/
â”‚           â””â”€â”€ Chat.tsx
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ postgres/
â”‚       â””â”€â”€ init.sql            # Full schema + RLS policies
â”œâ”€â”€ litellm/
â”‚   â””â”€â”€ config.yaml             # Model routing config (dev proxy)
â”œâ”€â”€ Archive/                    # Architecture + deployment specs
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## Development Guide

### Adding a new tool

1. Define it in [backend/app/agents/tools.py](backend/app/agents/tools.py):
```python
@tool
async def my_tool(param: str) -> str:
    """Describe what this tool does â€” the LLM reads this docstring."""
    return result
```
2. Add it to `ALL_TOOLS`. It's automatically available to the agent.

### Adding a new API route

1. Create `backend/app/api/my_feature.py`
2. Define a router with the standard pattern:
```python
router = APIRouter(prefix="/api/my-feature", tags=["my-feature"])

@router.get("/")
async def my_endpoint(user: CurrentUser = Depends(check_rate_limit)):
    ...
```
3. Register in [backend/app/main.py](backend/app/main.py): `app.include_router(my_feature.router)`

### Triggering episodic summarization

After a conversation ends, call:
```python
from app.workers.tasks import summarize_episode
summarize_episode.delay(conversation_id="...", user_id="...")
```
The Celery worker will summarize the conversation and store it â€” the agent will recall it automatically on the user's next session.

### Running schema migrations

```bash
# Inside the backend container or with DATABASE_URL set:
cd backend
alembic upgrade head

# Generate a new migration after schema changes:
alembic revision --autogenerate -m "describe your change"
```

### Monitoring tasks

Open **http://localhost:5555** (Flower) to see Celery task queues, retries, and status.

---

## Deployment

| Service | Provider | Notes |
|---|---|---|
| Backend + Celery | Railway | Single deploy, separate worker command |
| Frontend | Vercel | Auto-deploys on push to `main` |
| PostgreSQL | Neon | Managed, pgvector enabled |
| Redis | Upstash | Serverless |
| LiteLLM | N/A in prod | Use `LITELLM_MODE=library` â€” no container needed |

**Production environment changes:**
```bash
LITELLM_MODE=library          # eliminates the proxy container + network hop
DATABASE_URL=<neon-url>       # managed Postgres
REDIS_HOST=<upstash-host>
REDIS_PASSWORD=<upstash-pass>
JWT_SECRET=<openssl rand -hex 32>
ENVIRONMENT=production
```

CI/CD via GitHub Actions is configured in `.github/workflows/deploy.yml`.

---

## Roadmap

- **Phase 5:** Frontend auth UI, `/api/upload` route, admin dashboard, dev seed data, pytest suite
- **Future:** Scheduled Celery Beat (weekly memory consolidation, daily checkpoint cleanup), multi-agent support, tool allowlist per plan, pgvector partitioning at scale
